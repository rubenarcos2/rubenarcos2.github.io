<!DOCTYPE html>



<html lang="es">







<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Chatbot opensource</title>
  <meta name='description'
    content='Creación de un chatbot con modelo opensource. Aplicando la técnica RAG y utilizando Ollama, LangChain, ChromaDb y una salida final como API a un chatbot web'>

  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Dancing+Script%7CPT+Serif:400,400i,700%7CLato:400,700%7CRoboto:300,400"
    rel="stylesheet">

  <!-- Font Awesome v6.4.2 -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">

  <!-- Normalize v7.0.0 -->
  <link rel="stylesheet" href="/es/css/normalize.css">

  <!-- Main Stylesheet -->
  <link rel="stylesheet" href="/es/css/main.css">
  <link rel="canonical" href="https://rarcos.com/es/2024/05/25/Ollama_api_chatbot/">
  <link rel="alternate" type="application/rss+xml" title="Rubén Arcos" href="/es/feed.xml">

  <!-- Chrome, Firefox OS and Opera -->
  <meta name="theme-color" content="#faf5ee">
  <!-- Windows Phone -->
  <meta name="msapplication-navbutton-color" content="#faf5ee">
  <!-- iOS Safari -->
  <meta name="apple-mobile-web-app-status-bar-style" content="#faf5ee">
  <!-- Start cookieyes banner --> 
  <script id="cookieyes" type="text/javascript" src="https://cdn-cookieyes.com/client_data/6ae1ee7e40cef2c175ff9462/script.js"></script> 
  <!-- End cookieyes banner -->
</head>



  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VL018H86JE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-VL018H86JE');
</script> <!-- /Google Analytics -->
  

  <div class="full-page-container">
    <div class="overlay"></div>
    <header class="site-header clearfix">
  <div class="wrapper-content">

    <div class="switch-lang">
      
      
      
      <a id="switch-lang" href=" /2024/05/25/Ollama_api_chatbot/" title="Change language to en">
        <img src=" https://cdn.wpml.org/wp-content/plugins/sitepress-multilingual-cms/res/flags/en.png">
      </a>
      
      
      
      
      <!--es-->
      
      
    </div>

    <div class="navigation-wrap">
      <div class="nav-toggle-close"><i class="fa fa-times" aria-hidden="true"></i></div>

      <nav class="site-nav">
        
        
        
        
        
        
        
      
      
      <a href='/es/about/'>Sobre mí</a>
      
      
      
      
      
      
        
        
        
        
        
        
        
      
      
      <a href='/es/contacto/'>Contacto</a>
      
      
      
      
      
      
        
        
        
        
        
        
        
      
      
      <a href='/es/cookies/'>Cookies</a>
      
      
      
      
      
      
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <a href="https://linkedin.com/in/rubenarcos2"><i class="fa-brands fa-linkedin fa-2x" aria-hidden="true"></i></a>
        <a href="https://github.com/rubenarcos2"><i class="fa-brands fa-github fa-2x" aria-hidden="true"></i></a>
      </nav>

    </div>

    <span class="search-toggle">
      <i class="fa fa-search" aria-hidden="true"></i>
    </span>

    <div class="nav-toggle"><i class="fa fa-bars" aria-hidden="true"></i></div>

    <div class="site-logo">
      <a href="/es/">Rubén Arcos</a>
    </div>

  </div>
</header> <!-- /.site-header -->
    <div class="search-box">
  <div class="wrapper-content">
    <form class="search-form">
      
      
      
      <label class="screen-reader-text" for="search-input">Buscar...</label>
      <input type="text" id="search-input" class="search-field" name="s" placeholder="Buscar...">
      
      
      
      
      
      

    </form>
    <div class="btn-close">
      <span class="bt-search-close">
        <i class="fa fa-times" aria-hidden="true"></i>
      </span>
    </div>
  </div>
  <ul id="results-container" class="search-results-list"></ul>
</div> <!-- /.search-box -->

    <div class="post-header" style="background: url(/images/pages/ollama_api.jpg)">
  <div class="post-header-info">
    <h1 class="post-title">Chatbot opensource</h1>
    <div class="post-meta">
      
      
      
      <time datetime="2024-05-25T09:10:28+02:00">
         25/05/2024
      </time>
      
      
      
      
      
      
      
    </div>
  </div>
</div>
<div id="readPostContent" style="text-align:center"><a class="btn btn-middle" onclick="readPostContent(this)">
    
    
    
    Leer artículo en voz alta
    
    
    
    
    
    
  </a></div>
<div class="content-box">
  <article class="post">

    <div class="post-content">
      <p>La creación de un chatbot para una página web, es una tarea que se suele contratar a un proveedor de servicios, pero en este caso voy a contar como se puede realizar de principio a fin y además con un conjunto de herramientas que son libres y gratuitas.</p>

<h1 id="creación-de-un-chatbot-con-modelo-opensource">Creación de un chatbot con modelo opensource</h1>

<p>Los bot de charla o bot conversacional (en inglés: chatbot), son aplicaciones software que surgen en los años 60, y que simulan mantener una conversación con una persona al proveer respuestas automáticas, las cuales son previamente establecidas por un conjunto de expertos a entradas realizadas por el usuario. Estos bot, también conocidos como sistemas expertos, utilizan el razonamiento basado en casos (CBR: case base reasoning).</p>

<p>Habitualmente, la conversación se establece mediante texto, aunque también hay modelos que disponen de una interfaz de usuario multimedia que permiten la entrada auditiva. Más recientemente, algunos comienzan a utilizar programas conversores de texto a sonido (CTV), dotando de mayor realismo a la interacción con el usuario y ayudando a reducir el tiempo de respuesta.</p>

<p>Para establecer una conversación, han de utilizarse frases fácilmente comprensibles y que sean coherentes, aunque la mayoría de los bot conversacionales no consiguen comprender del todo. En su lugar, tienen en cuenta las palabras o frases del interlocutor, que les permitirán usar una serie de respuestas preparadas de antemano. Estos son capaces de reconocer la manera en la que una frase está formulada gracias a una serie de patrones comparativos preestablecidos, y de este modo, basándose en las diferentes variables de dicha frase, presentan una respuesta correspondiente. De esta manera, el bot es capaz de seguir una conversación con más o menos lógica, pero sin saber realmente de qué está hablando.</p>

<blockquote>
  <p>Para tener más información te recomiendo visites el siguiente <a href="https://es.wikipedia.org/wiki/Bot_conversacional">artículo</a> de wikipedia de donde se ha obtenido este fragmento.</p>
</blockquote>

<h1 id="qué-es-retrieval-augmented-generation-rag">¿Qué es Retrieval-Augmented Generation (RAG)?</h1>

<p>Es el aumento de conocimiento que se aplica a un LLM con datos que no ha sido entrenado. Esto permite al sistema de IA generativa proporcionar respuestas contextualmente adecuadas a las consultas, así como basar dichas respuestas en datos extremadamente recientes.</p>

<p>En pocas palabras, la RAG ayuda a los LLM a proporcionar respuestas más idóneas.</p>

<p><strong>Conclusiones clave</strong></p>

<ul>
  <li>La RAG es una técnica de inteligencia artificial relativamente nueva que mejora la calidad de la IA generativa al permitir a grandes modelos de lenguaje (LLM) aprovechar recursos de datos adicionales sin necesidad de volver a entrenarlos.</li>
  <li>Los modelos RAG crean repositorios de conocimientos basados en los datos de la propia organización. Estos repositorios se pueden actualizar continuamente para ayudar a la IA generativa a brindar respuestas adaptadas al contexto y oportunas.</li>
  <li>Los chatbots y otros sistemas conversacionales que utilizan el procesamiento del lenguaje natural pueden beneficiarse enormemente de la RAG y la IA generativa.</li>
  <li>La implementación de RAG requiere tecnologías como bases de datos vectoriales, que permiten la codificación rápida de nuevos datos y la búsqueda en esos datos para alimentar el LLM.</li>
</ul>

<h2 id="cómo-funciona">¿Cómo funciona?</h2>

<p>Los datos de esa biblioteca de conocimientos se procesan en representaciones numéricas utilizando un tipo especial de algoritmo llamado modelo de lenguaje embebido y se almacenan en una base de datos vectorial, en la que se puede buscar rápidamente para recuperar la información contextual correcta.</p>

<blockquote>
  <p>Para tener más información te recomiendo visites el siguiente <a href="https://www.oracle.com/es/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/">artículo</a> de Oracle de donde se ha obtenido este fragmento.</p>
</blockquote>

<h1 id="arquitectura">Arquitectura</h1>

<p><img src="/images/pages/ollama_api_arquitectura.jpg" alt="Architecture" title="Ollama API Architecture" /></p>

<p>Nosotros vamos a ejecutar nuestro modelo LLM en un servidor local. Además vamos a tener la posibilidad de cambiar de modelo desde una interfaz amigable de administración, que nos permite descargarnos el modelo que deseemos utilizar e incluso poder probarlo con antelación. Por ello, he elegido <a href="https://ollama.com/">Ollama</a> que tiene un <a href="https://ollama.com/library">conjunto de modelos</a> opensource bastante interesante. Vamos a ir viendo paso a paso cómo ir instalando y configurando nuestro software.</p>

<p>Pero antes de nada, vamos a ver una breve descripción de cada uno de los elementos que vamos a instalar:</p>

<h2 id="ollama">Ollama</h2>

<p>Ollama hace que sea muy fácil ejecutar LLM de código abierto localmente. Puede esperar un rendimiento decente incluso en portátiles pequeños. Ollama es una alternativa a Hugging Face para ejecutar modelos localmente. Las bibliotecas de Hugging Face se ejecutan sobre Tensorflow o Torch. Ollama usa llama.cpp como tiempo de ejecución subyacente. Esto hace que sea muy fácil comenzar con Ollama. Ni siquiera necesitas tener Python instalado.</p>

<h2 id="langchain">LangChain</h2>

<p>LangChain es un marco de trabajo de código abierto para crear aplicaciones basadas en modelos de lenguaje de gran tamaño (LLM). Los LLM son grandes modelos de aprendizaje profundo entrenados previamente con grandes cantidades de datos que pueden generar respuestas a las consultas de los usuarios, por ejemplo, responder preguntas o crear imágenes a partir de peticiones basadas en texto. LangChain proporciona herramientas y abstracciones para mejorar la personalización, precisión y relevancia de la información que generan los modelos. Por ejemplo, los desarrolladores pueden usar los componentes de LangChain para crear nuevas cadenas de peticiones o personalizar las plantillas existentes. LangChain también incluye componentes que permiten a los LLM acceder a nuevos conjuntos de datos sin necesidad de repetir el entrenamiento.</p>

<blockquote>
  <p>Para tener más información te recomiendo visites el siguiente <a href="https://aws.amazon.com/es/what-is/langchain/">artículo</a> de AWS de donde se ha obtenido este fragmento.</p>
</blockquote>

<h2 id="chromadb">ChromaDb</h2>

<p>ChromaDB es una base de datos especializada en el almacenamiento y recuperación eficiente de información lingüística, incluyendo datos de texto, anotaciones semánticas y sintácticas. ChromaDB es particularmente útil para el almacenamiento y la gestión de grandes cantidades de datos de lenguaje natural, lo que permite a los desarrolladores aprovechar al máximo los avances en algoritmos de aprendizaje automático y análisis de texto.</p>

<blockquote>
  <p>Para tener más información te recomiendo visites el siguiente <a href="https://brainq.ai/chromadb/">artículo</a> de donde se ha obtenido este fragmento.</p>
</blockquote>

<h2 id="api-con-flask">API con Flask</h2>

<p>Flask es un micro marco de trabajo web de Python que proporciona las herramientas necesarias para crear aplicaciones web de manera rápida y sencilla. Aunque es un micro marco, Flask es altamente modular y permite agregar fácilmente extensiones para agregar funcionalidades adicionales.</p>

<blockquote>
  <p>Para tener más información te recomiendo visites el siguiente <a href="https://datascientest.com/es/programacion-de-api-web-en-python-con-flask">artículo</a> de donde se ha obtenido este fragmento y donde puedes seguir el tutorial detallado de como crear una API con Flask.</p>
</blockquote>

<h2 id="el-chatbot-web">El Chatbot web</h2>

<p>Me he basado en esta integración de un chatbot web, para utilizar los recursos y adaptarlo a nuestro modelo opensource alojado en Ollama.</p>

<p>El código fuente se encuentra <a href="https://github.com/galaxyofai/chatgpt_flask_webapp">aquí</a>.</p>

<p>Y un explicación detallada de como integrarlo con otras API, se encuentra <a href="https://galaxyofai.com/building-a-flask-chat-web-app-with-openais-chatgpt-api/">aquí</a>.</p>

<h1 id="la-demostración">La demostración</h1>

<p>A continuación se ha embebido la aplicación realizada. Esta está alojada en un servidor de unas características muy limitadas para la ejecución de desarrollos de inteligencia artificial. Pero como demostración de que es posible, aquí se encuentra en funcionamiento:</p>

<blockquote>
  <p><img src="/images/pages/aviso_icon.png" alt="Aviso" /> <strong>El chatbot está alojado en un servidor con pocos recursos.</strong><br /><br />Por ello tarda bastante en contestar. Es precisamente lo que se busca, demostrar la viabilidad en este tipo de entornos.</p>
</blockquote>

<p><em>Haga clic sobre la burbuja morada, para abrir el Chatbot</em></p>

<p style="text-align: center">
  <iframe style="border: 0" id="chatbot" title="Chatbot" width="390" height="600" src="https://vps.rarcos.com:10453/">
  </iframe>
</p>

<h1 id="el-tutorial">El tutorial</h1>

<p>Como viene siendo habitual en esta página web, vamos a utilizar un entorno dockerizado que será el encargado de levantar Ollama, el administrador web de Ollama y nuestra aplicación de ChatBot web.</p>

<p>Para ello tenemos los siguiente scripts de <em>docker compose</em>:</p>

<h3 id="script-de-ejecución-más-completo-sin-gpu">Script de ejecución más completo (sin GPU)</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>version: '3.8'
services:
  app:
    build: .
    container_name: ollama-app
    ports:
      - 8002:8002
      - 5678:5678
    volumes:
      - ./app:/usr/src/app/
    command: python app.py
    restart: always
    depends_on:
      - ollama
      - ollama-webui
    networks:
      - ollama-docker

  ollama:
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    volumes:
      - .:/code
      - ./ollama/ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: always
    networks:
      - ollama-docker

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui
    volumes:
      - ./ollama/ollama-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 8080:8080
    environment:
      - '/ollama/api=http://ollama:11434/api'
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
    networks:
      - ollama-docker

networks:
  ollama-docker:
    external: false

</code></pre></div></div>

<h3 id="el-script-mínimo-sin-gpu">El script mínimo (sin GPU)</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>services:

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    tty: true
    restart: unless-stopped
    # Expose Ollama API outside the container stack
    ports:
      - 11434:11434
      - 53:53
    volumes:
      - ollama:/root/.ollama
    command: pip install -r requirements.txt

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 8080:8080
    environment:
      - "OLLAMA_API_BASE_URL=http://ollama:11434/api"
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped

volumes:
  ollama: {}
  open-webui: {}
</code></pre></div></div>
<h3 id="el-script-para-utilización-de-gpu">El script para utilización de GPU</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>version: '3.8'

services:
  app:
    build: .
    ports:
      - 8000:8000
      - 5678:5678
    volumes:
      - .:/code
    command: uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload
    restart: always
    depends_on:
      - ollama
      - ollama-webui
    networks:
      - ollama-docker
      
  ollama:
    volumes:
      - ./ollama/ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    networks:
      - ollama-docker
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui
    volumes:
      - ./ollama/ollama-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 8080:8080
    environment:
      - '/ollama/api=http://ollama:11434/api'
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
    networks:
      - ollama-docker

networks:
  ollama-docker:
    external: false

</code></pre></div></div>

<p>Las rutas de acceso que se establecen son:</p>
<ul>
  <li>Página web de administración de Ollama: <a href="http://localhost:8080">http://localhost:8080</a></li>
  <li>Motor de Ollana: <a href="http://localhost:11434">http://localhost:11434</a></li>
  <li>Aplicación web: <a href="http://localhost:8002">http://localhost:8002</a></li>
</ul>

<blockquote>
  <p>Para tener más información te recomiendo visites el siguiente <a href="https://github.com/valiantlynx/ollama-docker/">repositorio</a> de donde se ha obtenido este script y donde hay más ejemplos e información.</p>
</blockquote>

<h3 id="la-instalación-del-modelo-desde-el-administrador-web-de-ollama">La instalación del modelo desde el administrador web de Ollama</h3>

<p>Una vez tenemos el docker levantado, tan solo tenemos que entrar a la url <a href="http://localhost:8080">http://localhost:8080</a>, descargar e instalar el modelo.</p>

<p>A continuación se muestra un video de cómo se realiza:</p>

<p><img src="https://docs.openwebui.com/assets/images/demo-6793d95448aa180bca8dafbd21aa91b5.gif" alt="video" /></p>

<p>En nuestro caso introduciremos el modelo: <em>mistral:instruct</em></p>

<h3 id="la-aplicación-en-consola-el-chatbot">La aplicación en consola (el chatbot)</h3>

<p>Luego tenemos la aplicación en consola que consume el modelo LLM, en nuestro ejemplo se ha utilizado <em>Mistral</em> por sus gran rendimiento similar a Llama y por ser opensource. También tenemos la especificación del promt de consulta, con unas instrucciones previas a las consultas. Y el método de entrenamiento RAG.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from langchain_community.vectorstores import Chroma
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
import sys
 
class ChatWebDoc:
    vector_store = None
    retriever = None
    chain = None
 
    def __init__(self):
        self.model = ChatOllama(model="mistral:instruct")
        #Loading embedding
        self.embedding = FastEmbedEmbeddings()
 
        self.text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=100)
        self.prompt = ChatPromptTemplate.from_messages(
        [
            ("system", 
"""You are an assistant for question-answering tasks. Use only the following 
context to answer the question. If you don't know the answer, just say that you don't know.
 
CONTEXT:
 
{context}
"""),
            ("human", "{input}"),
        ]
    )
 
    def ingest(self, url_list):
        #Load web pages
        docs = WebBaseLoader(url_list).load()
        chunks = self.text_splitter.split_documents(docs)
 
        #Create vector store
        vector_store = Chroma.from_documents(documents=chunks, 
            embedding=self.embedding, persist_directory="./chroma_db")
 
    def load(self):
        #Load vector store
        vector_store = Chroma(persist_directory="./chroma_db", 
            embedding_function=self.embedding)
 
        #Create chain
        self.retriever = vector_store.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": 3,
                "score_threshold": 0.5,
            },
        )
 
        document_chain = create_stuff_documents_chain(self.model, self.prompt)
        self.chain = create_retrieval_chain(self.retriever, document_chain)
 
    def ask(self, query: str):
        if not self.chain:
            self.load()
 
        result = self.chain.invoke({"input": query})
 
        print(result["answer"])
        for doc in result["context"]:
            print("Source: ", doc.metadata["source"])
 
 
def build():
    w = ChatWebDoc()
    w.ingest([
        "https://www.webagesolutions.com/courses/WA3446-comprehensive-angular-programming",
        "https://www.webagesolutions.com/courses/AZ-1005-configuring-azure-virtual-desktop-for-the-enterprise",
        "https://www.webagesolutions.com/courses/AZ-305T00-designing-microsoft-azure-infrastructure-solutions",
        ])
 
def chat():
    w = ChatWebDoc()
 
    w.load()
 
    while True:
        query = input("&gt;&gt;&gt; ")
 
        if len(query) == 0:
            continue
 
        if query == "/exit":
            break
         
        w.ask(query)
 
if len(sys.argv) &lt; 2:
    chat()
elif sys.argv[1] == "--ingest":
    build()
</code></pre></div></div>

<p>La ejecución del entrenamiento RAG de las web provistas anteriorement en el programa anterior se realiza con el comando:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python rag-test.py --ingest
</code></pre></div></div>

<p>La interactuación con el model una vez entrenado:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python rag-test.py
</code></pre></div></div>

<blockquote>
  <p>Recomiendo ver con mayor detalle esta <a href="https://mobiarch.wordpress.com/2024/02/19/run-rag-locally-using-mistral-ollama-and-langchain/">página web</a> donde se trata esta misma aplicación.</p>
</blockquote>

<h2 id="la-aplicación-web-el-chatbot">La aplicación web (el Chatbot)</h2>

<p>Queda pendiente para una futura actualización del artículo que os muestre como integrar en la aplicación web el motor LLM, interactuar con el, entrenarlo con el método RAG y levantarlo en el servidor web. ¡Hasta pronto! 😉</p>

<hr />

<p><strong>Fuentes:</strong></p>

<ul>
  <li><a href="https://github.com/valiantlynx/ollama-docker/">https://github.com/valiantlynx/ollama-docker</a></li>
  <li><a href="https://mobiarch.wordpress.com/2024/02/19/run-rag-locally-using-mistral-ollama-and-langchain/">https://mobiarch.wordpress.com/2024/02/19/run-rag-locally-using-mistral-ollama-and-langchain/</a></li>
  <li><a href="https://realpython.com/build-llm-rag-chatbot-with-langchain/">https://realpython.com/build-llm-rag-chatbot-with-langchain/</a></li>
</ul>

    </div>

    <footer class="post-footer">
      
      
      
      <p>
      <h3>Licencia de contenido</h3>

      <p><img src="/images/pages/88x311.png" /></p>

      <p>Esta página web y todo su contenido, incluido proyectos y código fuente, está licenciado bajo una licencia de
        Creative Commons: Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) <a
          href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.es">Más info</a></p>

      </p>
      
      
      
      
      
      

      <ul class="post-footer-list">
        <li class="post-footer-item">
          <a class="post-facebook"
            href="https://www.facebook.com/sharer/sharer.php?u=https://rarcos.com/2024/05/25/Ollama_api_chatbot/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Facebook" rel="nofollow"><i class="fa-brands fa-facebook"
              aria-hidden="true"></i><span>Facebook</span></a>
        </li>

        <li class="post-footer-item">
          <a class="post-twitter"
            href="https://twitter.com/intent/tweet?text=Chatbot%20opensource&url=https://rarcos.com/2024/05/25/Ollama_api_chatbot/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Twitter" rel="nofollow"><i class="fa-brands fa-x-twitter"
              aria-hidden="true"></i><span>Twitter</span></a>
        </li>

        <li class="post-footer-item">
          <a href="http://pinterest.com/pin/create/button/?url=https://rarcos.com/2024/05/25/Ollama_api_chatbot/&amp;media=https://rarcos.com/images/pages/ollama_api.jpg&amp;description=Chatbot%20opensource"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            class="post-pinterest" title="Share on Pinterest" rel="nofollow"><i class="fa-brands fa-pinterest"
              aria-hidden="true"></i><span>Pinterest</span></a>
        </li>

        <li class="post-footer-item">
          <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://rarcos.com/2024/05/25/Ollama_api_chatbot/&title=Chatbot%20opensource"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            class="post-linkedin" title="Share on Linkedin" rel="nofollow"><i class="fa-brands fa-linkedin"
              aria-hidden="true"></i><span>Linkedin</span></a>
        </li>
      </ul>
      
      <div class="post-tag">
        <span>Tags:</span>
        
        <a href="/es/tags#ia" class="tag">ia</a>
        
        <a href="/es/tags#rag" class="tag">rag</a>
        
        <a href="/tags#open source" class="tag">open source</a>
        
        <a href="/es/tags#modelos" class="tag">modelos</a>
        
      </div>
      
    </footer>

    
    <div class="post-author">
      
      <div class="post-author-image">
        <a style="background-image: url(/images/ruben.jpg)">
          <span class="screen-reader-text">Rubén Arcos Picture</span>
        </a>
      </div>
      <div class="post-author-info">
        <h4>Rubén Arcos</h4>
        <p class="post-author-bio">
          
          
          
          Desarrollador TI. <br>Máster FP en IA y Big data, <br>Técnico superior en desarrollo de aplicaciones web y <br>Técnico superior en desarrollo de aplicaciones multiplataformas.
          
          
          
          
          
          
        </p>
        <div class="post-author-meta">
          
          <a href="https://twitter.com/rarcosdev" target="_blank"><i class="fa-brands fa-x-twitter" aria-hidden="true"></i></a>
          
          
          <a href="https://linkedin.com/in/rubenarcos2" target="_blank"><i class="fa-brands fa-linkedin" aria-hidden="true"></i></a>
          
          
          <a href="https://github.com/rubenarcos2" target="_blank"><i class="fa-brands fa-github" aria-hidden="true"></i></a>
          
          
          
          
          
          
          
          
          
        </div>
      </div>
      
    </div>

    <div class="page-navigation clearfix">
      
      <a class="prev-page" href="/es/2024/05/25/RAG_modelos_open_source/"><i class="fa fa-angle-left"
          aria-hidden="true"></i><span>RAG con modelos de IA open source</span></a>
      
      
    </div>

    
  </article> <!-- /.post -->

</div> <!-- /.content-box -->

    <div class="top" title="Scroll To Top">
  <i class="fa fa-chevron-up" aria-hidden="true"></i>
</div> <!-- /.top -->
    <footer class="site-footer">

  <div class="footer-wrapper">

    <h2 class="footer-heading">Rubén Arcos</h2>

    <ul class="social-footer">
    
      <li><a href="https://twitter.com/rarcosdev" target="_blank"><i class="fa-brands fa-x-twitter" aria-hidden="true"></i></a></li>
    
    
    
    
    <li><a href="https://linkedin.com/in/rubenarcos2" target="_blank"><i class="fa-brands fa-linkedin" aria-hidden="true"></i></a></li>
    
    
    <li><a href="https://github.com/rubenarcos2" target="_blank"><i class="fa-brands fa-github" aria-hidden="true"></i></a></li>
    
    
    
    
    
    
    
    
    
    </ul>

    <div class="copyright">
      <p><a href="https://jekyllrb.com/">Proudly powered by Jekyll</a> | Theme: Lokmont by <a href="https://artemsheludko.github.io/">Artem Sheludko</a> | Modifications: <a href="https://github.com/rubenarcos2">Rubén Arcos</a></p>
    </div>
  </div>

</footer> <!-- /.site-footer -->

  </div> <!-- /.full-page-container -->

  <script src="/js/jquery-3.7.1.min.js"></script>
<script src="/js/jquery.viewportchecker.min.js"></script>
<script src="/js/jquery.fitvids.js"></script>
<script src="/js/simple-jekyll-search.min.js"></script>
<script src="/js/transition.js"></script>
<script src="/js/zoom.min.js"></script>
<script src="/js/main.js"></script>
</body>

</html>