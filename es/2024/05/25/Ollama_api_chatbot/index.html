<!DOCTYPE html>



<html lang="es">







<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Chatbot opensource</title>
  <meta name='description'
    content='Creaci칩n de un chatbot con modelo opensource. Aplicando la t칠cnica RAG y utilizando Ollama, LangChain, ChromaDb y una salida final como API a un chatbot web'>

  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Dancing+Script%7CPT+Serif:400,400i,700%7CLato:400,700%7CRoboto:300,400"
    rel="stylesheet">

  <!-- Font Awesome v6.4.2 -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">

  <!-- Normalize v7.0.0 -->
  <link rel="stylesheet" href="/es/css/normalize.css">

  <!-- Main Stylesheet -->
  <link rel="stylesheet" href="/es/css/main.css">
  <link rel="canonical" href="https://rarcos.com/es/2024/05/25/Ollama_api_chatbot/">
  <link rel="alternate" type="application/rss+xml" title="Rub칠n Arcos" href="/es/feed.xml">

  <!-- Chrome, Firefox OS and Opera -->
  <meta name="theme-color" content="#faf5ee">
  <!-- Windows Phone -->
  <meta name="msapplication-navbutton-color" content="#faf5ee">
  <!-- iOS Safari -->
  <meta name="apple-mobile-web-app-status-bar-style" content="#faf5ee">
  <!-- Start cookieyes banner --> 
  <script id="cookieyes" type="text/javascript" src="https://cdn-cookieyes.com/client_data/6ae1ee7e40cef2c175ff9462/script.js"></script> 
  <!-- End cookieyes banner -->
</head>



  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VL018H86JE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-VL018H86JE');
</script> <!-- /Google Analytics -->
  

  <div class="full-page-container">
    <div class="overlay"></div>
    <header class="site-header clearfix">
  <div class="wrapper-content">

    <div class="switch-lang">
      
      
      
      <a id="switch-lang" href=" /2024/05/25/Ollama_api_chatbot/" title="Change language to en">
        <img src=" https://cdn.wpml.org/wp-content/plugins/sitepress-multilingual-cms/res/flags/en.png">
      </a>
      
      
      
      
      <!--es-->
      
      
    </div>

    <div class="navigation-wrap">
      <div class="nav-toggle-close"><i class="fa fa-times" aria-hidden="true"></i></div>

      <nav class="site-nav">
        
        
        
        
        
        
        
      
      
      <a href='/es/about/'>Sobre m칤</a>
      
      
      
      
      
      
        
        
        
        
        
        
        
      
      
      <a href='/es/contacto/'>Contacto</a>
      
      
      
      
      
      
        
        
        
        
        
        
        
      
      
      <a href='/es/cookies/'>Cookies</a>
      
      
      
      
      
      
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <a href="https://linkedin.com/in/rubenarcos2"><i class="fa-brands fa-linkedin fa-2x" aria-hidden="true"></i></a>
        <a href="https://github.com/rubenarcos2"><i class="fa-brands fa-github fa-2x" aria-hidden="true"></i></a>
      </nav>

    </div>

    <span class="search-toggle">
      <i class="fa fa-search" aria-hidden="true"></i>
    </span>

    <div class="nav-toggle"><i class="fa fa-bars" aria-hidden="true"></i></div>

    <div class="site-logo">
      <a href="/es/">Rub칠n Arcos</a>
    </div>

  </div>
</header> <!-- /.site-header -->
    <div class="search-box">
  <div class="wrapper-content">
    <form class="search-form">
      
      
      
      <label class="screen-reader-text" for="search-input">Buscar...</label>
      <input type="text" id="search-input" class="search-field" name="s" placeholder="Buscar...">
      
      
      
      
      
      

    </form>
    <div class="btn-close">
      <span class="bt-search-close">
        <i class="fa fa-times" aria-hidden="true"></i>
      </span>
    </div>
  </div>
  <ul id="results-container" class="search-results-list"></ul>
</div> <!-- /.search-box -->

    <div class="post-header" style="background: url(/images/pages/ollama_api.jpg)">
  <div class="post-header-info">
    <h1 class="post-title">Chatbot opensource</h1>
    <div class="post-meta">
      
      
      
      <time datetime="2024-05-25T09:10:28+02:00">
         25/05/2024
      </time>
      
      
      
      
      
      
      
    </div>
  </div>
</div>
<div id="readPostContent" style="text-align:center"><a class="btn btn-middle" onclick="readPostContent(this)">
    
    
    
    Leer art칤culo en voz alta
    
    
    
    
    
    
  </a></div>
<div class="content-box">
  <article class="post">

    <div class="post-content">
      <p>La creaci칩n de un chatbot para una p치gina web, es una tarea que se suele contratar a un proveedor de servicios, pero en este caso voy a contar como se puede realizar de principio a fin y adem치s con un conjunto de herramientas que son libres y gratuitas.</p>

<h1 id="creaci칩n-de-un-chatbot-con-modelo-opensource">Creaci칩n de un chatbot con modelo opensource</h1>

<p>Los bot de charla o bot conversacional (en ingl칠s: chatbot), son aplicaciones software que surgen en los a침os 60, y que simulan mantener una conversaci칩n con una persona al proveer respuestas autom치ticas, las cuales son previamente establecidas por un conjunto de expertos a entradas realizadas por el usuario. Estos bot, tambi칠n conocidos como sistemas expertos, utilizan el razonamiento basado en casos (CBR: case base reasoning).</p>

<p>Habitualmente, la conversaci칩n se establece mediante texto, aunque tambi칠n hay modelos que disponen de una interfaz de usuario multimedia que permiten la entrada auditiva. M치s recientemente, algunos comienzan a utilizar programas conversores de texto a sonido (CTV), dotando de mayor realismo a la interacci칩n con el usuario y ayudando a reducir el tiempo de respuesta.</p>

<p>Para establecer una conversaci칩n, han de utilizarse frases f치cilmente comprensibles y que sean coherentes, aunque la mayor칤a de los bot conversacionales no consiguen comprender del todo. En su lugar, tienen en cuenta las palabras o frases del interlocutor, que les permitir치n usar una serie de respuestas preparadas de antemano. Estos son capaces de reconocer la manera en la que una frase est치 formulada gracias a una serie de patrones comparativos preestablecidos, y de este modo, bas치ndose en las diferentes variables de dicha frase, presentan una respuesta correspondiente. De esta manera, el bot es capaz de seguir una conversaci칩n con m치s o menos l칩gica, pero sin saber realmente de qu칠 est치 hablando.</p>

<blockquote>
  <p>Para tener m치s informaci칩n te recomiendo visites el siguiente <a href="https://es.wikipedia.org/wiki/Bot_conversacional">art칤culo</a> de wikipedia de donde se ha obtenido este fragmento.</p>
</blockquote>

<h1 id="qu칠-es-retrieval-augmented-generation-rag">쯈u칠 es Retrieval-Augmented Generation (RAG)?</h1>

<p>Es el aumento de conocimiento que se aplica a un LLM con datos que no ha sido entrenado. Esto permite al sistema de IA generativa proporcionar respuestas contextualmente adecuadas a las consultas, as칤 como basar dichas respuestas en datos extremadamente recientes.</p>

<p>En pocas palabras, la RAG ayuda a los LLM a proporcionar respuestas m치s id칩neas.</p>

<p><strong>Conclusiones clave</strong></p>

<ul>
  <li>La RAG es una t칠cnica de inteligencia artificial relativamente nueva que mejora la calidad de la IA generativa al permitir a grandes modelos de lenguaje (LLM) aprovechar recursos de datos adicionales sin necesidad de volver a entrenarlos.</li>
  <li>Los modelos RAG crean repositorios de conocimientos basados en los datos de la propia organizaci칩n. Estos repositorios se pueden actualizar continuamente para ayudar a la IA generativa a brindar respuestas adaptadas al contexto y oportunas.</li>
  <li>Los chatbots y otros sistemas conversacionales que utilizan el procesamiento del lenguaje natural pueden beneficiarse enormemente de la RAG y la IA generativa.</li>
  <li>La implementaci칩n de RAG requiere tecnolog칤as como bases de datos vectoriales, que permiten la codificaci칩n r치pida de nuevos datos y la b칰squeda en esos datos para alimentar el LLM.</li>
</ul>

<h2 id="c칩mo-funciona">쮺칩mo funciona?</h2>

<p>Los datos de esa biblioteca de conocimientos se procesan en representaciones num칠ricas utilizando un tipo especial de algoritmo llamado modelo de lenguaje embebido y se almacenan en una base de datos vectorial, en la que se puede buscar r치pidamente para recuperar la informaci칩n contextual correcta.</p>

<blockquote>
  <p>Para tener m치s informaci칩n te recomiendo visites el siguiente <a href="https://www.oracle.com/es/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/">art칤culo</a> de Oracle de donde se ha obtenido este fragmento.</p>
</blockquote>

<h1 id="arquitectura">Arquitectura</h1>

<p><img src="/images/pages/ollama_api_arquitectura.jpg" alt="Architecture" title="Ollama API Architecture" /></p>

<p>Nosotros vamos a ejecutar nuestro modelo LLM en un servidor local. Adem치s vamos a tener la posibilidad de cambiar de modelo desde una interfaz amigable de administraci칩n, que nos permite descargarnos el modelo que deseemos utilizar e incluso poder probarlo con antelaci칩n. Por ello, he elegido <a href="https://ollama.com/">Ollama</a> que tiene un <a href="https://ollama.com/library">conjunto de modelos</a> opensource bastante interesante. Vamos a ir viendo paso a paso c칩mo ir instalando y configurando nuestro software.</p>

<p>Pero antes de nada, vamos a ver una breve descripci칩n de cada uno de los elementos que vamos a instalar:</p>

<h2 id="ollama">Ollama</h2>

<p>Ollama hace que sea muy f치cil ejecutar LLM de c칩digo abierto localmente. Puede esperar un rendimiento decente incluso en port치tiles peque침os. Ollama es una alternativa a Hugging Face para ejecutar modelos localmente. Las bibliotecas de Hugging Face se ejecutan sobre Tensorflow o Torch. Ollama usa llama.cpp como tiempo de ejecuci칩n subyacente. Esto hace que sea muy f치cil comenzar con Ollama. Ni siquiera necesitas tener Python instalado.</p>

<h2 id="langchain">LangChain</h2>

<p>LangChain es un marco de trabajo de c칩digo abierto para crear aplicaciones basadas en modelos de lenguaje de gran tama침o (LLM). Los LLM son grandes modelos de aprendizaje profundo entrenados previamente con grandes cantidades de datos que pueden generar respuestas a las consultas de los usuarios, por ejemplo, responder preguntas o crear im치genes a partir de peticiones basadas en texto. LangChain proporciona herramientas y abstracciones para mejorar la personalizaci칩n, precisi칩n y relevancia de la informaci칩n que generan los modelos. Por ejemplo, los desarrolladores pueden usar los componentes de LangChain para crear nuevas cadenas de peticiones o personalizar las plantillas existentes. LangChain tambi칠n incluye componentes que permiten a los LLM acceder a nuevos conjuntos de datos sin necesidad de repetir el entrenamiento.</p>

<blockquote>
  <p>Para tener m치s informaci칩n te recomiendo visites el siguiente <a href="https://aws.amazon.com/es/what-is/langchain/">art칤culo</a> de AWS de donde se ha obtenido este fragmento.</p>
</blockquote>

<h2 id="chromadb">ChromaDb</h2>

<p>ChromaDB es una base de datos especializada en el almacenamiento y recuperaci칩n eficiente de informaci칩n ling칲칤stica, incluyendo datos de texto, anotaciones sem치nticas y sint치cticas. ChromaDB es particularmente 칰til para el almacenamiento y la gesti칩n de grandes cantidades de datos de lenguaje natural, lo que permite a los desarrolladores aprovechar al m치ximo los avances en algoritmos de aprendizaje autom치tico y an치lisis de texto.</p>

<blockquote>
  <p>Para tener m치s informaci칩n te recomiendo visites el siguiente <a href="https://brainq.ai/chromadb/">art칤culo</a> de donde se ha obtenido este fragmento.</p>
</blockquote>

<h2 id="api-con-flask">API con Flask</h2>

<p>Flask es un micro marco de trabajo web de Python que proporciona las herramientas necesarias para crear aplicaciones web de manera r치pida y sencilla. Aunque es un micro marco, Flask es altamente modular y permite agregar f치cilmente extensiones para agregar funcionalidades adicionales.</p>

<blockquote>
  <p>Para tener m치s informaci칩n te recomiendo visites el siguiente <a href="https://datascientest.com/es/programacion-de-api-web-en-python-con-flask">art칤culo</a> de donde se ha obtenido este fragmento y donde puedes seguir el tutorial detallado de como crear una API con Flask.</p>
</blockquote>

<h2 id="el-chatbot-web">El Chatbot web</h2>

<p>Me he basado en esta integraci칩n de un chatbot web, para utilizar los recursos y adaptarlo a nuestro modelo opensource alojado en Ollama.</p>

<p>El c칩digo fuente se encuentra <a href="https://github.com/galaxyofai/chatgpt_flask_webapp">aqu칤</a>.</p>

<p>Y un explicaci칩n detallada de como integrarlo con otras API, se encuentra <a href="https://galaxyofai.com/building-a-flask-chat-web-app-with-openais-chatgpt-api/">aqu칤</a>.</p>

<h1 id="la-demostraci칩n">La demostraci칩n</h1>

<p>A continuaci칩n se ha embebido la aplicaci칩n realizada. Esta est치 alojada en un servidor de unas caracter칤sticas muy limitadas para la ejecuci칩n de desarrollos de inteligencia artificial. Pero como demostraci칩n de que es posible, aqu칤 se encuentra en funcionamiento:</p>

<blockquote>
  <p><img src="/images/pages/aviso_icon.png" alt="Aviso" /> <strong>El chatbot est치 alojado en un servidor con pocos recursos.</strong><br /><br />Por ello tarda bastante en contestar. Es precisamente lo que se busca, demostrar la viabilidad en este tipo de entornos.</p>
</blockquote>

<p><em>Haga clic sobre la burbuja morada, para abrir el Chatbot</em></p>

<p style="text-align: center">
  <iframe style="border: 0" id="chatbot" title="Chatbot" width="390" height="600" src="https://vps.rarcos.com:10453/">
  </iframe>
</p>

<h1 id="el-tutorial">El tutorial</h1>

<p>Como viene siendo habitual en esta p치gina web, vamos a utilizar un entorno dockerizado que ser치 el encargado de levantar Ollama, el administrador web de Ollama y nuestra aplicaci칩n de ChatBot web.</p>

<p>Para ello tenemos los siguiente scripts de <em>docker compose</em>:</p>

<h3 id="script-de-ejecuci칩n-m치s-completo-sin-gpu">Script de ejecuci칩n m치s completo (sin GPU)</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>version: '3.8'
services:
  app:
    build: .
    container_name: ollama-app
    ports:
      - 8002:8002
      - 5678:5678
    volumes:
      - ./app:/usr/src/app/
    command: python app.py
    restart: always
    depends_on:
      - ollama
      - ollama-webui
    networks:
      - ollama-docker

  ollama:
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    volumes:
      - .:/code
      - ./ollama/ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: always
    networks:
      - ollama-docker

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui
    volumes:
      - ./ollama/ollama-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 8080:8080
    environment:
      - '/ollama/api=http://ollama:11434/api'
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
    networks:
      - ollama-docker

networks:
  ollama-docker:
    external: false

</code></pre></div></div>

<h3 id="el-script-m칤nimo-sin-gpu">El script m칤nimo (sin GPU)</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>services:

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    tty: true
    restart: unless-stopped
    # Expose Ollama API outside the container stack
    ports:
      - 11434:11434
      - 53:53
    volumes:
      - ollama:/root/.ollama
    command: pip install -r requirements.txt

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 8080:8080
    environment:
      - "OLLAMA_API_BASE_URL=http://ollama:11434/api"
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped

volumes:
  ollama: {}
  open-webui: {}
</code></pre></div></div>
<h3 id="el-script-para-utilizaci칩n-de-gpu">El script para utilizaci칩n de GPU</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>version: '3.8'

services:
  app:
    build: .
    ports:
      - 8000:8000
      - 5678:5678
    volumes:
      - .:/code
    command: uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload
    restart: always
    depends_on:
      - ollama
      - ollama-webui
    networks:
      - ollama-docker
      
  ollama:
    volumes:
      - ./ollama/ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    networks:
      - ollama-docker
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui
    volumes:
      - ./ollama/ollama-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 8080:8080
    environment:
      - '/ollama/api=http://ollama:11434/api'
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
    networks:
      - ollama-docker

networks:
  ollama-docker:
    external: false

</code></pre></div></div>

<p>Las rutas de acceso que se establecen son:</p>
<ul>
  <li>P치gina web de administraci칩n de Ollama: <a href="http://localhost:8080">http://localhost:8080</a></li>
  <li>Motor de Ollana: <a href="http://localhost:11434">http://localhost:11434</a></li>
  <li>Aplicaci칩n web: <a href="http://localhost:8002">http://localhost:8002</a></li>
</ul>

<blockquote>
  <p>Para tener m치s informaci칩n te recomiendo visites el siguiente <a href="https://github.com/valiantlynx/ollama-docker/">repositorio</a> de donde se ha obtenido este script y donde hay m치s ejemplos e informaci칩n.</p>
</blockquote>

<h3 id="la-instalaci칩n-del-modelo-desde-el-administrador-web-de-ollama">La instalaci칩n del modelo desde el administrador web de Ollama</h3>

<p>Una vez tenemos el docker levantado, tan solo tenemos que entrar a la url <a href="http://localhost:8080">http://localhost:8080</a>, descargar e instalar el modelo.</p>

<p>A continuaci칩n se muestra un video de c칩mo se realiza:</p>

<p><img src="https://docs.openwebui.com/assets/images/demo-6793d95448aa180bca8dafbd21aa91b5.gif" alt="video" /></p>

<p>En nuestro caso introduciremos el modelo: <em>mistral:instruct</em></p>

<h3 id="la-aplicaci칩n-en-consola-el-chatbot">La aplicaci칩n en consola (el chatbot)</h3>

<p>Luego tenemos la aplicaci칩n en consola que consume el modelo LLM, en nuestro ejemplo se ha utilizado <em>Mistral</em> por sus gran rendimiento similar a Llama y por ser opensource. Tambi칠n tenemos la especificaci칩n del promt de consulta, con unas instrucciones previas a las consultas. Y el m칠todo de entrenamiento RAG.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from langchain_community.vectorstores import Chroma
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
import sys
 
class ChatWebDoc:
    vector_store = None
    retriever = None
    chain = None
 
    def __init__(self):
        self.model = ChatOllama(model="mistral:instruct")
        #Loading embedding
        self.embedding = FastEmbedEmbeddings()
 
        self.text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=100)
        self.prompt = ChatPromptTemplate.from_messages(
        [
            ("system", 
"""You are an assistant for question-answering tasks. Use only the following 
context to answer the question. If you don't know the answer, just say that you don't know.
 
CONTEXT:
 
{context}
"""),
            ("human", "{input}"),
        ]
    )
 
    def ingest(self, url_list):
        #Load web pages
        docs = WebBaseLoader(url_list).load()
        chunks = self.text_splitter.split_documents(docs)
 
        #Create vector store
        vector_store = Chroma.from_documents(documents=chunks, 
            embedding=self.embedding, persist_directory="./chroma_db")
 
    def load(self):
        #Load vector store
        vector_store = Chroma(persist_directory="./chroma_db", 
            embedding_function=self.embedding)
 
        #Create chain
        self.retriever = vector_store.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": 3,
                "score_threshold": 0.5,
            },
        )
 
        document_chain = create_stuff_documents_chain(self.model, self.prompt)
        self.chain = create_retrieval_chain(self.retriever, document_chain)
 
    def ask(self, query: str):
        if not self.chain:
            self.load()
 
        result = self.chain.invoke({"input": query})
 
        print(result["answer"])
        for doc in result["context"]:
            print("Source: ", doc.metadata["source"])
 
 
def build():
    w = ChatWebDoc()
    w.ingest([
        "https://www.webagesolutions.com/courses/WA3446-comprehensive-angular-programming",
        "https://www.webagesolutions.com/courses/AZ-1005-configuring-azure-virtual-desktop-for-the-enterprise",
        "https://www.webagesolutions.com/courses/AZ-305T00-designing-microsoft-azure-infrastructure-solutions",
        ])
 
def chat():
    w = ChatWebDoc()
 
    w.load()
 
    while True:
        query = input("&gt;&gt;&gt; ")
 
        if len(query) == 0:
            continue
 
        if query == "/exit":
            break
         
        w.ask(query)
 
if len(sys.argv) &lt; 2:
    chat()
elif sys.argv[1] == "--ingest":
    build()
</code></pre></div></div>

<p>La ejecuci칩n del entrenamiento RAG de las web provistas anteriorement en el programa anterior se realiza con el comando:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python rag-test.py --ingest
</code></pre></div></div>

<p>La interactuaci칩n con el model una vez entrenado:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python rag-test.py
</code></pre></div></div>

<blockquote>
  <p>Recomiendo ver con mayor detalle esta <a href="https://mobiarch.wordpress.com/2024/02/19/run-rag-locally-using-mistral-ollama-and-langchain/">p치gina web</a> donde se trata esta misma aplicaci칩n.</p>
</blockquote>

<h2 id="la-aplicaci칩n-web-el-chatbot">La aplicaci칩n web (el Chatbot)</h2>

<p>Queda pendiente para una futura actualizaci칩n del art칤culo que os muestre como integrar en la aplicaci칩n web el motor LLM, interactuar con el, entrenarlo con el m칠todo RAG y levantarlo en el servidor web. 춰Hasta pronto! 游땔</p>

<hr />

<p><strong>Fuentes:</strong></p>

<ul>
  <li><a href="https://github.com/valiantlynx/ollama-docker/">https://github.com/valiantlynx/ollama-docker</a></li>
  <li><a href="https://mobiarch.wordpress.com/2024/02/19/run-rag-locally-using-mistral-ollama-and-langchain/">https://mobiarch.wordpress.com/2024/02/19/run-rag-locally-using-mistral-ollama-and-langchain/</a></li>
  <li><a href="https://realpython.com/build-llm-rag-chatbot-with-langchain/">https://realpython.com/build-llm-rag-chatbot-with-langchain/</a></li>
</ul>

    </div>

    <footer class="post-footer">
      
      
      
      <p>
      <h3>Licencia de contenido</h3>

      <p><img src="/images/pages/88x311.png" /></p>

      <p>Esta p치gina web y todo su contenido, incluido proyectos y c칩digo fuente, est치 licenciado bajo una licencia de
        Creative Commons: Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) <a
          href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.es">M치s info</a></p>

      </p>
      
      
      
      
      
      

      <ul class="post-footer-list">
        <li class="post-footer-item">
          <a class="post-facebook"
            href="https://www.facebook.com/sharer/sharer.php?u=https://rarcos.com/2024/05/25/Ollama_api_chatbot/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Facebook" rel="nofollow"><i class="fa-brands fa-facebook"
              aria-hidden="true"></i><span>Facebook</span></a>
        </li>

        <li class="post-footer-item">
          <a class="post-twitter"
            href="https://twitter.com/intent/tweet?text=Chatbot%20opensource&url=https://rarcos.com/2024/05/25/Ollama_api_chatbot/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Twitter" rel="nofollow"><i class="fa-brands fa-x-twitter"
              aria-hidden="true"></i><span>Twitter</span></a>
        </li>

        <li class="post-footer-item">
          <a href="http://pinterest.com/pin/create/button/?url=https://rarcos.com/2024/05/25/Ollama_api_chatbot/&amp;media=https://rarcos.com/images/pages/ollama_api.jpg&amp;description=Chatbot%20opensource"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            class="post-pinterest" title="Share on Pinterest" rel="nofollow"><i class="fa-brands fa-pinterest"
              aria-hidden="true"></i><span>Pinterest</span></a>
        </li>

        <li class="post-footer-item">
          <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://rarcos.com/2024/05/25/Ollama_api_chatbot/&title=Chatbot%20opensource"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            class="post-linkedin" title="Share on Linkedin" rel="nofollow"><i class="fa-brands fa-linkedin"
              aria-hidden="true"></i><span>Linkedin</span></a>
        </li>
      </ul>
      
      <div class="post-tag">
        <span>Tags:</span>
        
        <a href="/es/tags#ia" class="tag">ia</a>
        
        <a href="/es/tags#rag" class="tag">rag</a>
        
        <a href="/tags#open source" class="tag">open source</a>
        
        <a href="/es/tags#modelos" class="tag">modelos</a>
        
      </div>
      
    </footer>

    
    <div class="post-author">
      
      <div class="post-author-image">
        <a style="background-image: url(/images/ruben.jpg)">
          <span class="screen-reader-text">Rub칠n Arcos Picture</span>
        </a>
      </div>
      <div class="post-author-info">
        <h4>Rub칠n Arcos</h4>
        <p class="post-author-bio">
          
          
          
          Desarrollador TI. <br>M치ster FP en IA y Big data, <br>T칠cnico superior en desarrollo de aplicaciones web y <br>T칠cnico superior en desarrollo de aplicaciones multiplataformas.
          
          
          
          
          
          
        </p>
        <div class="post-author-meta">
          
          <a href="https://twitter.com/rarcosdev" target="_blank"><i class="fa-brands fa-x-twitter" aria-hidden="true"></i></a>
          
          
          <a href="https://linkedin.com/in/rubenarcos2" target="_blank"><i class="fa-brands fa-linkedin" aria-hidden="true"></i></a>
          
          
          <a href="https://github.com/rubenarcos2" target="_blank"><i class="fa-brands fa-github" aria-hidden="true"></i></a>
          
          
          
          
          
          
          
          
          
        </div>
      </div>
      
    </div>

    <div class="page-navigation clearfix">
      
      <a class="prev-page" href="/es/2024/05/25/RAG_modelos_open_source/"><i class="fa fa-angle-left"
          aria-hidden="true"></i><span>RAG con modelos de IA open source</span></a>
      
      
    </div>

    
  </article> <!-- /.post -->

</div> <!-- /.content-box -->

    <div class="top" title="Scroll To Top">
  <i class="fa fa-chevron-up" aria-hidden="true"></i>
</div> <!-- /.top -->
    <footer class="site-footer">

  <div class="footer-wrapper">

    <h2 class="footer-heading">Rub칠n Arcos</h2>

    <ul class="social-footer">
    
      <li><a href="https://twitter.com/rarcosdev" target="_blank"><i class="fa-brands fa-x-twitter" aria-hidden="true"></i></a></li>
    
    
    
    
    <li><a href="https://linkedin.com/in/rubenarcos2" target="_blank"><i class="fa-brands fa-linkedin" aria-hidden="true"></i></a></li>
    
    
    <li><a href="https://github.com/rubenarcos2" target="_blank"><i class="fa-brands fa-github" aria-hidden="true"></i></a></li>
    
    
    
    
    
    
    
    
    
    </ul>

    <div class="copyright">
      <p><a href="https://jekyllrb.com/">Proudly powered by Jekyll</a> | Theme: Lokmont by <a href="https://artemsheludko.github.io/">Artem Sheludko</a> | Modifications: <a href="https://github.com/rubenarcos2">Rub칠n Arcos</a></p>
    </div>
  </div>

</footer> <!-- /.site-footer -->

  </div> <!-- /.full-page-container -->

  <script src="/js/jquery-3.7.1.min.js"></script>
<script src="/js/jquery.viewportchecker.min.js"></script>
<script src="/js/jquery.fitvids.js"></script>
<script src="/js/simple-jekyll-search.min.js"></script>
<script src="/js/transition.js"></script>
<script src="/js/zoom.min.js"></script>
<script src="/js/main.js"></script>
</body>

</html>